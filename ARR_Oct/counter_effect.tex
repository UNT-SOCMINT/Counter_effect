% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{ACL2023}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{verbatim}
\usepackage{array}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage{graphicx}
\usepackage{amsmath} 
\usepackage{fixltx2e}
\usepackage{paralist}

\newcommand\ddaauxdown{\rotatebox[origin=c]{-90}{\scalebox{0.90}{$\dashrightarrow$}}} 
\newcommand\dashdownarrow{\mathrel{\text{\ddaauxdown}}}
\newcommand\ddaauxup{\rotatebox[origin=c]{90}{\scalebox{0.90}{$\dashrightarrow$}}} 
\newcommand\dashuparrow{\mathrel{\text{\ddaauxup}}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Hate Cannot Drive out Hate:\\Forecasting Conversation Incivility of Replies to Hateful Content}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{First Author \\
	Affiliation / Address line 1 \\
	Affiliation / Address line 2 \\
	Affiliation / Address line 3 \\
	\texttt{email@domain} \\\And
	Second Author \\
	Affiliation / Address line 1 \\
	Affiliation / Address line 2 \\
	Affiliation / Address line 3 \\
	\texttt{email@domain} \\}

\begin{document}
	\maketitle
	\begin{abstract}
		User-generated counter hate speech is a promising means to combat hate speech,
		but questions about whether it can stop incivility in follow-up conversations linger.
		We argue that what matters is stopping incivility from emerging in follow-up conversations---counter hate speech that elicits more incivility is counterproductive.
		In this study, we introduce the task of predicting incivility in the follow-up conversation for a reply to hateful content.
		The conversation incivility is measured based on the discourse following a reply: number of civil and uncivil comments published by each user.
		A linguistic analysis draws insights into the differences in the language of replies with high and low incivility.
		Experimental results show that forecasting incivility is challenging.
		We close with a qualitative analysis shedding light into the most common errors made by the best model.
	\end{abstract}
	
	\section{Introduction}
	The pervasive problem of online hate speech has motivated
	researchers to investigate methods for mitigating hatred.
	For example, hate speech detection has received considerable attention~\cite{schmidt-wiegand-2017-survey,10.1145/3232676}. 
	Counter hate speech, which is a ``direct response that counters hate speech''~\cite{DBLP:conf/icwsm/MathewSTRSMG019},
	is a remedy to address hate speech~\cite{richards2000counterspeech}. 
	Unlike content moderation,
	counter hate does not interfere with the principle of free and open public spaces for debate~\cite{DBLP:conf/icwsm/MathewSTRSMG019,schieb2016governing,chung-etal-2019-conan}.
	
	\begin{figure}
		\small
		\centering
		\begin{tabular}{@{}p{\columnwidth}@{}}
			\toprule
			Hateful post:
			\emph{Just curious how you can identify with a movement which has essentially become a hate group full of crazy feminists.}  \\ \addlinespace
			(Ineffective) counter hate post:
			\emph{Come on man, most feminists are ok. Hate group? how can you use such a strong term?}  \\ \addlinespace
			Hateful post (after ineffective counter hate speech):
			\emph{No, it’s not strong. Don’t lie through your teeth. Let me know when you want to talk, c**t.} \\ 
			\bottomrule
		\end{tabular}
		\caption{
			An excerpt from a Reddit conversation.
			The second post contains counter hate speech but it elicits additional uncivil behaviors.
			Indeed, the third post escalates the hate with respect to the original hateful post.
		}
		\label{f:problem-example}
	\end{figure}
	
	Social media platforms like Facebook have started counter hate programs.\footnote{\url{https://counterspeech.fb.com/en/}} 
	Recently, the NLP community has focused on counter hate speech detection~\cite{DBLP:conf/icwsm/MathewSTRSMG019,yu-etal-2022-hate} and generation~\cite{tekiroglu-etal-2020-generating,fanton-etal-2021-human,zhu-bhat-2021-generate}.
	These and other previous efforts (Section \ref{s:related_work}) make the following assumption:
	counter hate speech is an ideal solution to stop or at least mitigate online incivility.
	While intuitive, we are not aware of strong evidence supporting this assumption.
	Consider the Reddit conversation in Figure \ref{f:problem-example}.\footnote{The examples in this paper contain hateful content. We cannot avoid it due to the nature of our work.}
	The first post is hateful towards feminists in general. 
	The second post is a strong counter hate argument if we ignore the follow-up conversation.
	As strong as it might be, however, it elicits additional incivility: the next post escalates the hateful content further by attacking the author.
	Even if human intuitions are consistent with ground truth at most times, could we endow artificial systems to depict the future trajectory of conversations for replies to hateful content?
	
	In this paper, we aim to computationally forecast conversation incivility of replies to hateful content.
	We do not limit ourselves to focusing on counter hate speech. 
	Rather, we look at all replies with varying conversation outcomes. 
	Regardless of the content---short or long, offensive or polite, well-argued or fatally flawed from a logic standpoint---we consider the outcome of a reply is civil if the discourse that follows is primarily not uncivil.
	Our rationale is that what matters is preventing uncivil content from emerging in the follow-up conversation rather than coming up with elaborate counter hate arguments.
	Further, we argue that looking at genuine online discourse and assessing what comments elicit additional uncivil behaviors---even if they are well-meaning and polished counter hate arguments---is a worthwhile goal.
	
	We focus on incivility as the conversation outcome and investigate how linguistic discourse is tied to the future trajectory of a conversation. 
	Recent studies in measuring conversation incivility use number or ratio of uncivil comments~\cite{DBLP:conf/icwsm/LiuGHC18,10.1145/3366423.3380273,DBLP:conf/kdd/DahiyaSSGCEMB021,garland2022impact}.
	In contrast, we quantify conversation incivility by also considering thread lengths~\cite{tsagkias2009predicting,Yano_Smith_2010,artzi-etal-2012-predicting} and user re-entry behaviors~\cite{10.1145/2433396.2433401}, which build the bridge between works on conversation modeling in general and in incivility domains.
	Additionally, the work presented here could complement current studies on counter hate speech by 
	investigating a richer source of replies to hate 
	and 
	providing insights into counter hate strategies.
	
	We study this problem with a new dataset of Reddit conversations.\footnote{Data and code available at anonymous.link}
	We propose a new metric that takes the number of both uncivil and civil comments into account.
	Our metric also consider whether the follow-up comments are from many different "drive-by" commenters or from relatively few people with repeated engagement~\cite{10.1145/2433396.2433401}. 
	We argue that our metric is more sound in many scenarios.
	For example, for a reply to hateful content followed by five uncivil posts, the conversation incivility is higher than that followed by only one uncivil post,
	while the ratio of uncivil comments are the same.
	For the same reply, if the five uncivil posts are from different users, the conversation incivility is higher than that where all of them are from the same user,
	while the number of uncivil comments are the same.
	
	Armed with the Reddit conversations and our metric,
	we first conduct a linguistic analysis to analyze the characteristics of replies to hateful posts with varying degrees of conversation incivility (namely high, medium or low incivility).
	We then experiment with classifiers to identify whether a reply will be high, medium or low incivility.
	Our models obtain modest results,
	and we present a qualitative analysis describing the most common error types.
	
	We answer the following questions:
	\begin{compactenum}
		\item Are conversation incivility of all replies to hateful posts equal? (they aren't);
		\item Do replies that have high, medium, and low conversation incivility use different language? (they do);
		\item Do models to assess the conversation incivility benefit from having access to the hateful post in addition to the reply? (they do);
		\item When differentiating between the top-$k$ most and least uncivil replies,
		is it true that the smaller the $k$ the easier the task? (it is).
	\end{compactenum}
	
	
	\section{Related Work}
	\label{s:related_work}
	\paragraph{Hate speech}
	has been an active research area in recent years~\cite{fortuna2018survey}. 
	Most prior work focuses on detecting whether existing content is hateful.
	A few corpora have been curated for hate speech detection from diverse sources such as
	Twitter~\cite{waseem-hovy-2016-hateful, hateoffensive},
	Yahoo!~\cite{nobata2016abusive},
	Fox News~\cite{gao-huang-2017-detecting},
	Gab~\cite{DBLP:conf/aaai/MathewSYBG021},
	and Reddit~\cite{qian-etal-2019-benchmark}.
	
	There are several efforts on forecasting whether online content will result in additional uncivil behaviors. 
	Incivility is a broader concept than hate speech, which includes name-calling, personal attacks, racism/sexism, abusive languages, and vulgarity~\cite{sadeque-etal-2019-incivility,davidson-etal-2020-developing}.
	\citet{10.1145/2998181.2998213} predict whether a moderator will flag a post for removal.
	\citet{zhang-etal-2018-conversations} predict whether a comment-reply pair at the very beginning of a conversation will lead to a personal attack.
	\citet{DBLP:conf/icwsm/LiuGHC18} forecast whether an Instagram post will receive more than $n$ uncivil comments.
	\citet{DBLP:conf/kdd/DahiyaSSGCEMB021} use a tweet and a few of its replies to forecast the incivility score of upcoming replies.
	They calculate incivility score by adding the probability from a toxicity classifier and the presence of uncivil words.
	Similarily, our work aims to forecast conversation incivility in following discourse; 
	however, we measure incivility by also considering civil comments as well as user re-entry behaviors
	and 
	we focus on replies to hateful content instead of those at the beginning of a conversation.
	
	\paragraph{Counter hate speech}
	Prior work on counter hate speech has contributed several corpora with counter hate content~\cite{DBLP:conf/icwsm/MathewSTRSMG019,qian-etal-2019-benchmark,chung-etal-2019-conan,yu-etal-2022-hate}.
	~\citet{chung-etal-2019-conan} collect synthetic counter hate speech generated on-demand by trained operators.
	Researchers have mainly relied on this corpora for counter hate speech  generation~\cite{tekiroglu-etal-2020-generating,fanton-etal-2021-human,zhu-bhat-2021-generate,ashida-komachi-2022-towards}. 
	Compared to genuine counter hate speech written by regular people out of their own desires and motivations, synthetic counter hate speech is not as rich (e.g., \emph{This kind of language is inappropriate and should be avoided}).
	Also, we are unaware of the outcomes of synthetic counter hate speech in real online conversations.
	Some user studies that investigate the outcome of counter hate speech are lack in scale, they compare the control group with the treatment group that has received interventions~\cite{munger2017tweetment,hangartner2021empathy,bilewicz2021artificial}. 
	The only large-scale work is by \citet{garland2022impact}.
	They work with German tweets and estimate the impact of each reply by comparing the 
	average incivility scores of all content before and after.
	While we are inspired by previous approaches to address online hatred, we
	(a) analyze user-generated content that does not induce uncivil behaviors \emph{in real online conversations}, and
	(b) measure incivility automatically and therefore bypass the burden of manual annotations.
	
	\paragraph{Conversational forecasting}
	Besides conversation incivility, previous work has explored a rich source of different conversation outcomes, such as betrayal in games~\cite{niculae-etal-2015-linguistic}, success in persuading others~\cite{10.1145/2872427.2883081}, and the winner of a debate~\cite{potash-rumshisky-2017-towards}.
	We build on prior work in modeling conversation trajectory from the structural aspects of conversation~\cite{10.1145/2433396.2433401} and complement them by also considering the linguistic aspects.
	
	
	
	\section{Measuring Conversation Incivility} %.Measuring Effectiveness}
	\label{s:metric}
	We propose a new metric to measure the conversation incivility of a reply $r$ to hateful content.
	Our metric consists of two main components: uncivil behaviors $U(r)$ and civil behaviors $C(r)$ .
	All the comments in the follow-up conversation that after a reply $r$ are from a population of unique users $P$.
	For each user $i$ in $P$ (for $i = 1,2,...,k$), let $n_{ui}$ denote the number of uncivil comments the user $P_i$ posts, and $n_{ci}$ denote the number of civil comments.
	Thus, uncivil behaviors $U(r)$ is measured by summing up the square root of $n_{ui}$ for all the users in the conversation after $r$.
	Similarly, civil behaviors $C(r)$ is measure by the summation of the square root of $n_{ci}$ for all the users.
	Intuitively, the conversation incivility of a reply $r$ is higher when there are more uncivil comments and they are from different commenters, and fewer civil comments from relatively few people.
	We formally define the conversation incivility score of $r$ as follows:
	$$S(r) = \alpha U(r) - (1-\alpha) C(r)$$
	Where
	\[
	\begin{cases}
		 U(r) =	\sum_{i=1}^{k} \sqrt{n_{ui}} \\
	     C(r) =	\sum_{i=1}^{k} \sqrt{n_{ci}} 
	\end{cases}
	\]
	
	The parameter $\alpha$ determines how much weight to give to each component.
	The smaller $\alpha$ is, the more important uncivil behaviors is (i.e., more civil comments are needed to neutralize the incivility degree of one uncivil comment).
	We also believe the future trajectory of a conversation becomes more toxic and unhealthy if it involves a larger number of participants venting or misbehaving, and this should be treated differently from that where there are only few people with repeated engagement~\cite{10.1145/2433396.2433401}.
	Finally, if a reply to hateful content elicits a long and civil comment thread, then it is the one that attracts a great deal of attention and promotes healthy discussions. 
	Our metric builds on prior work on comment-volume prediction~\cite{artzi-etal-2012-predicting,10.1145/2433396.2433401} and we adapt it in modeling conversation incivility.
	
	
	\section{A Corpus of Online Hate, Replies, and their Conversation Incivility}
	\label{s:corpus}
	We choose Reddit as the starting point for our corpus.
	The PushShift API makes it possible to retrieve whole conversation threads seamlessly.\footnote{\url{https://pushshift.io/api-parameters/}}
	As the prevalence of online hate in the wild is very low (0.1\% in English language social media~\cite{vidgen-etal-2019-challenges}), many studies use keyword sampling to increase the chances of finding hateful content. 
	Keywords, however, may introduce topic and author biases~\cite{wiegand-etal-2019-detection,vidgen-etal-2021-introducing}. 
	In this study, we use community-based sampling and identify
	%, similar to \citet{qian-etal-2019-benchmark} and \citet{vidgen-etal-2021-introducing}. 
	%We identify
	39 subreddits (see the full list in Appendix \ref{sec:appendix_data}) that are thought to be hateful~\cite{qian-etal-2019-benchmark,guest-etal-2021-expert,vidgen-etal-2021-introducing}. 
	This includes subreddits such as \textit{r/MensRights},  \textit{r/PurplePillDebate}, \textit{r/ImGoingToHellForThis}, and \textit{r/Seduction}.
	We retrieve a total of 1,382,596 comments from 5,325 submissions.
	
	The next steps are to 
	(a)~identify the comments that are hateful and their replies~(Section \ref{ss:identify}),
	and
	(b)~assess the conversation incivility of replies to hateful content~(Section \ref{ss:assess}).
	As we shall see, the second step requires identifying uncivil content in the comments following the reply.
	
	\paragraph{Identifying Hate Comments and Their Replies}
	\label{ss:identify}
	We identify hateful content in the 1,382,596 comments using pre-trained models \cite{DBLP:journals/corr/abs-1907-11692} with the corpus by~\citet{qian-etal-2019-benchmark}
	and
	the implementation by~\citet{phang2020jiant}.
	We made this choice for several reasons.
	First, the corpus annotates Reddit comments as hateful or not hateful, the same domain we work with.
	Second, the classifier obtains outstanding results: 0.93 F1.
	In a more strict evaluation using Cohen's $\kappa$, we obtain $\kappa=0.83$ between the predictions and the gold annotations in the test set.
	Note that $\kappa$ coefficients above $0.80$ indicate (almost) perfect agreement~\cite{artstein2008inter}.
	In other words, the predictions of hateful posts are reliable enough to be considered as ground truth.
	
	After automatically identifying hateful comments, we pair
	(a) each hateful comment with each of its direct replies
	and
	(b) each reply to a hateful comment with all future comments in the same thread.
	We found 21,845  hate comments in the 39 subreddits we work with.
	On average, a hate comment has 1.56 direct replies,
	and there are 2.36 comments published after each reply.

	
	\paragraph{Assessing Conversation Incivility}
	\label{ss:assess}
	The metric to measure the conversation incivility of a reply $r$ requires us to get
	how many civil and uncivil comments are published after $r$ for each user.
	The calculation of civil comments is a simple count of the civil comments.
	Compared with hateful comments, uncivil comments include broader cases~\cite{davidson-etal-2020-developing}.
	We therefore calculate uncivil comments based on the output of three classifiers.
	We build three models by training the same architecture as that in Section \ref{ss:identify} with the corpus by \citet{qian-etal-2019-benchmark}
	and two additional corpora~\cite{hateoffensive,vidgen-etal-2021-introducing}.
	We consider a comment published after a reply as uncivil if any of the three classifiers predicts \emph{uncivil}. 
	
	After calculating civil behaviors and uncivil behaviors for each reply,
	calculating the conversation incivility score $S(r)$ is straightforward.
	We experiment with $\alpha = 0.8$, 
	as we believe that uncivil content is more critical and several civil messages are needed to neutralize uncivil content in Reddit.
	We do not claim that our setting is the only option or the best one. 
	We note that absolute incivility scores are not as important as relative values.
	We use the scores to group all replies based on quantiles (top 25\%, middle 50\%, and bottom 25\%). 
	The categories show as following:
	\begin{compactitem}
		\item \emph{High} with $ S(r) \in [16.26,0.22) $;
		\item \emph{Medium} with $ S(r) \in [0.22, -0.20) $; and
		\item \emph{Low} with  $ S(r) \in [-0.20, -48.76]$.
	\end{compactitem}

	We refer to this grouping (high, medium or low) as the conversation incivility level of a reply.
	Note that 95\% of replies with \emph{Medium} conversation incivility have no follow-up discourse ($S(r)=0$).   
	Replies with \emph{High} conversation incivility always have at least one uncivil comment after $r$.
	We will explore the differences in the use of language between replies with \emph{High} and \emph{Low} conversational incivility in Section~\ref{s:corpus_analysis}.
	
	
	
	\paragraph{Manual Validation}
	\label{ss:validation}
	We manually validate the soundness of our metric with a sample of 500 replies (250 with high incivility and 250 with low incivility).
	First, we select Reddit snippets containing 
	(a) the hateful comment,
	(b)	the reply,	and 
    (c) all comments published after the reply.
	Second, we pair snippets where the reply has high incivility with that has low incivility (250 pairs).
	Third, we show the snippets to the annotators
	and ask them which one of the two replies has higher conversational incivility.
	We skip validating replies with medium incivility as most of them (95\%) have no following discourses, making the task quite easy.
	We hire two research assistants as annotators.
	Both annotators agree with the label obtained by the incivility scores on more than 85\% of the pairs,
	and Cohen's $\kappa$ is above 0.70, which are considered substantial agreement.

	
	\section{Corpus Analysis} 
	\label{s:corpus_analysis}
	
	Table \ref{t:label-distribution} presents distribution of conversation incivility levels in our corpus.
	We include in the high incivility and low incivility all replies with the threshold incivility scores.
	Almost half of the replies to a hateful comment have medium conversation incivility~(49.5\%).
	The number of replies with high conversation incivility (25.2\%) and low conversation incivility (25.3\%) are very close.
	We note that these percentages are encouraging, as these subreddits are known to harvest conversations about hateful topics.
	
	We show examples of each incivility level in Table \ref{t:corpus-examples}. 
	In the first example,
	the reply shows disagreement by attacking the author of the hate comment (\emph{little man}).
	It leads to high conversation incivility: 26 out of 33 comments that follow the reply are uncivil.
	Indeed, we observe the follow-up conversation is made up of behaviors where the original authors of the hateful comment and the reply  repetitively denigrating each other.
	In the second example, the reply uses sarcasm to stop arguing with the hateful comment  (``Have a nice day!'').
	There are no comments after this reply, yielding a 0 incivility score.
	It has \emph{medium} conversation incivility---no additional uncivil comments is posted.
	In the third example, the reply denounces the misbehavior in the hate comment is inappropriate without getting into details or personal attacks (``[...] It is not perfectly okay.'').
	This strategy is a common counter hate strategy~\cite{DBLP:conf/icwsm/MathewSTRSMG019} and the follow-up conversation is barely uncivil: although 3 comments are uncivil, they are posted by the same user and 29 comments from 24 users remain civil.
	
	\begin{table}
		\centering
		\begin{tabular}{lrrrr}
			\toprule
			& \multicolumn{3}{c}{Incivility level} \\ \cmidrule{2-4} 
			\multicolumn{1}{c}{} & High & Medium & Low & All \\
			\hline
			\addlinespace[1pt]
			\# & 8,587 & 16,884 & 8,644 & 34,115 \\ 
			\% & 25.2 & 49.5 & 25.3 & 100 \\
			\bottomrule
		\end{tabular}
		\caption{Label distribution of incivility levels.}
		\label{t:label-distribution}
	\end{table}
	
	\begin{table}[t]
		\small
		\centering
		\begin{tabular}{p{7.2cm}}
			\toprule
			Hate: \emph{Lol this thread is full of internet losers like you.} \\
			Reply: \emph{Ha! You don't even know me little man.}\\ \addlinespace
			$S(r) = 5.02$, High incivility\\ 	
			\midrule
			Hate: \emph{Too easy to trigger you maga f**ks, snowflake.} \\
			Reply: \emph{Have a nice day!}\\ \addlinespace
			$S(r) = 0$, Medium incivility\\ 					
			\midrule
			Hate: \emph{Trash talking is perfectly ok. He is an a**hole and clearly cannot take it for sh*t.} \\
			Reply: \emph{It's *tolerable* under those conditions. It is not perfectly okay.}\\ \addlinespace
			$S(r) = -3.92$, Low incivility\\ 
			\bottomrule
		\end{tabular}
		
		\caption{Examples from our corpus and their incivility levels.
			We also include their incivility scores.}
		\label{t:corpus-examples}
	\end{table}

		
					% Start: Table 
	\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
	\begin{table*}
		\setlength{\tabcolsep}{.075in}
		\centering
		\small
		\begin{tabular}{l P{.5in}P{.5in}P{.5in}P{.5in}P{.5in}P{.5in}}
			\toprule
			& All & Discussion & Hobby & Identity & Meme & Media\\ \midrule
			\textbf{Textual factors}\\
			~~~Tokens & $\uparrow\uparrow\uparrow$ & $\uparrow\uparrow\uparrow$ & $\underline\uparrow\underline\uparrow$ & $\uparrow\uparrow\uparrow$ & $\uparrow\uparrow\uparrow$ & $\uparrow\uparrow\uparrow$ \\ 
			~~~Negations & $\uparrow\uparrow\uparrow$ & $\uparrow\uparrow\uparrow$ & $\underline\uparrow$ &  $\uparrow\uparrow\uparrow$ &  $\uparrow\uparrow\uparrow$ & $\underline\uparrow\underline\uparrow\underline\uparrow$ \\
			~~~1st person pronouns & $\uparrow\uparrow\uparrow$ & $\underline\uparrow\underline\uparrow\underline\uparrow$ & $\underline\uparrow$ &  $\uparrow\uparrow\uparrow$ & $\uparrow\uparrow\uparrow$ & $\underline\uparrow\underline\uparrow$ \\
			~~~2nd person pronouns & $\uparrow\uparrow\uparrow$ & $\uparrow\uparrow\uparrow$ & $\uparrow\uparrow\uparrow$ & $\uparrow\uparrow\uparrow$ & $\uparrow\uparrow\uparrow$ & $\uparrow\uparrow\uparrow$ \\
			~~~Named entity (norp) & $\uparrow\uparrow\uparrow$ & $\uparrow\uparrow\uparrow$ & $\underline\uparrow$ & $\underline\uparrow$ & & $\underline\uparrow$ \\
			~~~Question marks & $\uparrow\uparrow\uparrow$ & $\underline\uparrow$ &  & $\underline\uparrow\underline\uparrow\underline\uparrow$ & $\underline\uparrow$ & $\underline\uparrow$ \\
			~~~Quotation marks & $\uparrow\uparrow\uparrow$  & $\uparrow\uparrow\uparrow$  & $\underline\uparrow$ & $\underline\uparrow\underline\uparrow$ &  & $\underline\uparrow$ \\
			\midrule
			\textbf{Sentiment factors}\\
			~~~Positive words & $\downarrow\downarrow\downarrow$ & $\downarrow\downarrow\downarrow$ & $\underline\downarrow\underline\downarrow\underline\downarrow$ & $\downarrow\downarrow\downarrow$ & $\underline\downarrow\underline\downarrow$ & $\underline\downarrow\underline\downarrow$ \\
			~~~Negative words & $\uparrow\uparrow\uparrow$ & $\uparrow\uparrow\uparrow$ & $\uparrow\uparrow\uparrow$ & $\uparrow\uparrow\uparrow$ & $\uparrow\uparrow\uparrow$ & $\uparrow\uparrow$ \\
			~~~Disgust words & $\uparrow\uparrow\uparrow$ & $\underline\uparrow\underline\uparrow\underline\uparrow$ & $\underline\uparrow\underline\uparrow\underline\uparrow$ & $\uparrow\uparrow\uparrow$  &  $\underline\uparrow$ & $\underline\uparrow$ \\
			~~~Hatred words & $\uparrow\uparrow\uparrow$ & $\underline\uparrow$ & & $\underline\uparrow\underline\uparrow$ & &  $\underline\uparrow\underline\uparrow\underline\uparrow$ \\
			~~~Angry words &  $\uparrow\uparrow\uparrow$ & $\underline\uparrow\underline\uparrow$ & $\underline\uparrow$ & $\underline\uparrow\underline\uparrow$ & $\underline\uparrow$ & $\underline\uparrow\underline\uparrow$ \\
			\bottomrule
		\end{tabular}
		
		\caption{Linguistic analysis comparing the replies to hateful content that have high and low conversation incivility.
			We also provide results of replies in each category of subreddits.
			Number of arrows indicate the p-value (t-test; one: $p<0.05$, two: $p<0.01$, and three: $p<0.001$).
			Arrow direction indicates whether higher values correlate with high (up) or low (down) incivility. 
			Tests that do not pass the Bonferroni correction are underlined.}
		\label{t:linguistic}
	\end{table*}
	% End: Table 
	
	\paragraph{Linguistic insights} 
	We perform a linguistic analysis to shed light on differences in the language people use in the replies with high conversation incivility and low conversation incivility.
	As different communities vary in the topics, content, rules, etc., 
	we further conduct analyses in each type of communities to see whether the differences between high and low incivility still exist.
	The 39 subreddits are grouped into five categories based on the taxonomy from~\citet{Weld_Zhang_Althoff_2022}.
	The five categories we are using are: \emph{Discussion}, \emph{Hobby}, \emph{Identity}, \emph{Meme}, and \emph{Media-sharing} (see Appendix~\ref{sec:appendix_data}).
	
	All factors we consider are based on counts of
	(a)~textual features (top block)
	or
	(b) sentiment words presence.
	For quotation marks, we use the character  '>' and the text that follows overlaps with the hateful comment~\cite{chakrabarty-etal-2019-ampersand,jo-etal-2020-detecting}.
	We check for negation cues using the list by~\citet{fancellu-etal-2016-neural}. 
	We use spaCy to check for the mentions of named entities.\footnote{\url{https://spacy.io/usage/linguistic-features}}
	For sentiment and cognition, we use the Sentiment Analysis and Cognition Engine (SEANCE) lexicon,
	a tool for psychological linguistic analysis \cite{crossley2017sentiment}. 
	Statistical tests are conducted using unpaired t-tests between the groups, of which the replies have high or low conversation incivility.
	We draw several interesting insights:
	\begin{compactitem}
		\item Regarding textual factors for all the replies, the more tokens, negations, pronouns (1st and 2nd person), entities of nationalities or religious or political groups, question and quotation marks indicate that the reply has high conversation incivility.
		As negation cues are often used to dispute the hateful comment, the future trajectory of conversation has a higher chance to turn uncivil.
		\item Regarding sentiment, there are significant differences in the uses of positive and negative words. Replies that use more hatred, disgust and angry words are more inclined to tie with a higher incivility degree in the follow-up conversations.
		\item Although topics, content and rules vary across communities, we observe consistency in the differences between high and low incivility. The more 2nd pronouns referring to the author of the hateful comment and the more negative words in the replies, the higher conversation incivility they have.
	\end{compactitem}
	
	
	
	\section{Experiments and Results} 
	\label{s:experiments}
	
	We experiment with models to solve two problems:
	\begin{compactitem}
		\item Determining the conversation incivility level of a reply to hateful content: high, medium or low incivility (Section \ref{ss:levels}); and
		\item Differentiating the top-$k$\% and bottom-$k$\% replies according to their conversation incivility scores (Section \ref{ss:topbottom}).
	\end{compactitem}
	
	
	All our models are neural classifiers with the RoBERTa transformer~\cite{DBLP:journals/corr/abs-1907-11692} as the main component.
	We use the pretrained models by HuggingFace~\cite{wolf-etal-2020-transformers}
	and Pytorch~\cite{NEURIPS2019_9015} to implement our models.
	The supplementary materials provide details.
	
		% Start: Table 
	\begin{table*}[ht!]
		\setlength{\tabcolsep}{.0775in}
		\small
		\centering	
		\begin{tabular}{l ccc ccc ccc ccc}
			\toprule
			\multicolumn{1}{c}{} & \multicolumn{3}{c}{High} & \multicolumn{3}{c}{Medium} & \multicolumn{3}{c}{Low} & \multicolumn{3}{c}{Weighted Average} \\
			\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} 
			& P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 \\
			\hline
			\addlinespace[1pt]
			Majority Baseline & 0.00 & 0.00 & 0.00 & 0.49 & 1.00 & 0.66 & 0.00 & 0.00 & 0.00 &  0.24 & 0.49 & 0.32 \\ \addlinespace
			
			RoBERTa classifier with \\ 
			~~~hate comment & 0.34 & 0.31 & 0.33 & 0.53 & 0.72 & 0.61 & 0.27 & 0.10 & 0.15 & 0.41 & 0.46 & 0.42 \\ \addlinespace
			
			~~~reply & 0.42 & 0.33 & 0.37 & 0.53 & 0.77 & 0.63 & 0.29 & 0.10 & 0.15 & 0.44 & 0.49 & 0.44\\		
			~~~~~~+ blending & 0.42 & 0.42 & 0.42 & 0.55 & 0.76 & 0.63 & 0.33 & 0.08 & 0.13 & 0.46 & 0.50 & 0.45\\
			~~~~~~+ pretraining† & 0.44 & 0.37 & 0.40 & 0.56 & 0.72 & 0.63 & 0.32 & 0.20 & 0.25 & 0.47 & 0.50 & 0.47\\ 
			~~~~~~~~~+ blending†‡ & 0.47 & 0.39 & 0.43 & 0.57 & 0.70 & 0.63 & 0.34 & 0.25 & 0.29 & 0.49 & 0.51 & 0.49\\ 
			\addlinespace
			
			~~~hate comment + reply & 0.43 & 0.32 & 0.36 & 0.55 & 0.66 & 0.60 & 0.32 & 0.27 & 0.29 & 0.46 & 0.48 & 0.46\\
			~~~~~~+ blending† & 0.43 & 0.37 & 0.40 & 0.54 & 0.79 & 0.64 & 0.35 & 0.10 & 0.16 & 0.47 & 0.51 & 0.46\\
			~~~~~~+ pretraining†‡ & \textbf{0.52} & \textbf{0.43} & \textbf{0.47} & \textbf{0.59} & \textbf{0.77}& \textbf{0.67} & 0.42 & 0.23 & 0.30 & \textbf{0.53 }& \textbf{0.55} & \textbf{0.52}\\
			~~~~~~~~~+ blending†‡ & 0.48 & 0.42 & 0.45 & 0.59 & 0.72 & 0.65 & \textbf{0.38} & \textbf{0.27} & \textbf{0.32} & \textbf{0.51} & \textbf{0.53} & \textbf{0.52}\\
			\bottomrule
			
		\end{tabular}
		\caption{Results obtained with several models. 
			We indicate statistical significance (McNemar’s test \cite{mcnemar1947note} over the weighted average) as follows: 
			† indicates statistically significant ($p<0.05$) results with respect to the \emph{reply} model, ‡ with respect to the \emph{hate comment + reply} model.
			Training with the \emph{hate comment + reply}
			coupled with pretraining with stance or both pretraining and blending stance yields the best results (F1: 0.52).
			The supplementary materials detail the results pretraining with and blending all related tasks we consider.}
		\label{t:model-results}
	\end{table*}
	
	\begin{table*}
		\small
		\centering	
		\begin{tabular}{lrl  cccc cccc ccc}
			\toprule
			\multicolumn{3}{c}{} & \multicolumn{3}{c}{Top-k\%} && \multicolumn{3}{c}{Bottom-k\%} && \multicolumn{3}{c}{Weighted Average}  \\ \cmidrule(lr){4-6} \cmidrule(lr){8-10} \cmidrule(lr){12-14} 
			& Size && P & R & F1 && P & R & F1 && P & R & F1  \\ \midrule
			$\mathit{k}=5$  &  3,234 &&0.77 & 0.74 & 0.76 && 0.72 & 0.75 & 0.74	&& 0.75 & 0.75 & 0.75 \\
			$\mathit{k}=10$ &  6,124 && 0.74 & 0.77 & 0.76 && 0.68 & 0.65 & 0.67 && 0.72 & 0.72 & 0.72 \\
			$\mathit{k}=15$ & 10,434 && 0.70 & 0.76 & 0.73 && 0.68 & 0.61 & 0.65 && 0.69 & 0.69 & 0.69 \\
			$\mathit{k}=20$ & 17,231 && 0.67 & 0.63 & 0.65 && 0.65 & 0.68 & 0.66  && 0.66 & 0.66 & 0.66 \\ \bottomrule
			
		\end{tabular}
		\caption{Experimental results differentiating the top-$k$\% and bottom-$k$\% replies to hateful content according to their incivility scores.
			We present results for several values of $k$.
			The results are higher than when also identifying replies with \emph{medium} incivility.
			Additionally, it is easier to differentiate the replies that have the very top and bottom of the incivility scores: the smaller the $k$, the higher the weighted average.
		}
		\label{t:model-optimalk}
	\end{table*}
	
	\subsection{Determining Incivility Level}
	\label{ss:levels}
	
	%\paragraph{Neural Network Architecture} 
	We experiment with neural classifiers built on top of RoBERTa~\cite{DBLP:journals/corr/abs-1907-11692}. 
	The neural architecture consists of the RoBERTa transformer, 
	a fully connected layer (768 neurons and \texttt{tanh} activation), 
	and 
	another fully connected layer (3 neurons and softmax activation) to make predictions (high, medium, or low incivility). 
	To investigate whether adding the hate comment would be beneficial, we consider three textual inputs:
	\begin{compactitem}
		\item the hate comment;
		\item the reply to the hate comment; and
		\item the hate comment and the reply.
	\end{compactitem}
	
	Intuitively, the reply is the most important input, but as we shall see including the hate comment is beneficial.
	We concatenate both inputs with the \texttt{[SEP]} special token. 
	
	\noindent
	\textbf{Pretraining with Related Tasks} 
	We experiment with several corpora to investigate whether pretraining with related tasks is beneficial. 
	Specifically, we pretrain with existing corpora annotating: 
	(a) hate speech: hateful or not hateful~\cite{hateoffensive}; 
	(b) sentiment: negative, neutral, or positive \cite{rosenthal-etal-2017-semeval}; 
	(c) sarcasm: sarcasm or not sarcasm \cite{ghosh-etal-2020-report}; 
	(d) counter hate speech: hate, neutral, or counterhate \cite{yu-etal-2022-hate}; and
	(e) stance: agree, neutral, or attack \cite{pougubiyong2021debagreement}.
	

	
	\noindent
	\textbf{Blending Additional Annotations}
	Pretraining takes place prior to training with our corpus.
	We also experiment with a complementary approach: blending additional corpora during the training process, as proposed by \citet{shnarch-etal-2018-will}.
	With blending, there are two phases in the training process: 
	(a) $\mathit{m}$ blending epochs using all of our corpus and a fraction of an additional corpus,
	and
	(b) $\mathit{n}$ epochs using only our corpus. 
	In each blending epoch, a random fraction of an additional corpus is fed to the network.
	The fraction is determined by a blending factor $\alpha \in [0..1]$. 
	The first blending epoch is trained with our corpus and the whole additional corpus.
	Subsequent blending epochs use smaller fractions of the additional corpus.
	We use for blending purposes the corpora we use for pretraining that annotate three labels~\cite{rosenthal-etal-2017-semeval,pougubiyong2021debagreement,yu-etal-2022-hate}.


	% Start: Table 
	\begin{table*}
		\small
		\centering
		\begin{tabular}{@{\hspace{.03in}}p{2.5cm}p{0.1cm}p{8.4cm}ll@{\hspace{.03in}}}
			\toprule
			Error Type & \% & Example & Ground Truth  & Predicted  \\ \midrule
			
			Rhetorical question & 23 & Hate: \emph{Why not complain about women you sad pathetic f**k?} &  & \\
			&    & Reply: \emph{Haha another b**ch a** white knight. F**k off sissy.} & Medium & High  \\
			\midrule
			Uncivil but low incivility & 18 & Hate: \emph{I've addressed this about forty times with as many smooth brains as you so. You're an idiot.} & & \\
			&   &  Reply: \emph{I think you might be the idiot here retard.} & Low & High \\
			\midrule
			Civil but high incivility & 16 & Hate: \emph{You’re an ignorant twat who just parrots what they read in FB and reddit memes. [\ldots] What a cancer you are.}  & & \\
			&  & Reply: \emph{Calling others cancer is taking it too far. Mind rule 4, please.} & High & Low\\
			\midrule
			Sarcasm or irony & 15 & Hate: \emph{No you retard, where is the f**king lie?} & & \\
			&  & Reply: \emph{Name calling nice argument.} & High & Low \\
			\midrule
			General knowledge & 10 & Hate: \emph{lol bet you thought a single thing you said wasn’t retarded.} & & \\
			&  & Reply: \emph{This place is infested with incels and TD trolls. } & High & Medium \\
			\midrule
			Negation & 8 & Hate: \emph{Why we have to tolerate Islam? They call us filth. Christians are horrible as well. Both are f**king awful.} && \\
			& & Reply: \emph{Not all Muslims are bigots, like not all Christians are bigots.} & Medium & High \\
			\bottomrule
		\end{tabular}
		\caption{Most common error types made by the best model (predictions by hate comment + reply + pretraining).
		}
		\label{t:error}
	\end{table*}
	% End: Table 
	
	\subsubsection{Quantitative Results}
	We split the 34,115 replies in our corpus into training (60\%), development (20\%) and testing (20\%).
	We present results with the testing split in Table \ref{t:model-results}.
	The majority baseline always predicts \emph{medium}.
	The remaining rows present results with different settings:
	using as input the \emph{hate comment}, the \emph{reply} or both without pretraining or blending, and also with
	pretraining, blending and both.
	We provide here results pretraining and blending with the most beneficial tasks:
	counter hate speech (+ pretraining when using reply, and + blending using hate comment + reply),
	and stance for the remaining ones.
	We tune the blending factor $\alpha$  with the training and development splits, like other hyperparameters.
	We found the optimal $\alpha$ to be 0.5 when only blending and 1.0 when pretraining and blending. 
	
	Using only the reply as input is a strong baseline: it substantially outperforms the majority baseline (F1: 0.44 vs. 0.32).
	Using both the hate comment and reply yields better results (F1: 0.46).
	Pretraining and blending yield better results compared with blending alone (reply: 0.49 vs. 0.45, hate comment + reply: 0.52 vs. 0.46).
	Also, pretraining or pretraining and blending are more beneficial when the input is both the hate comment and the reply.
	Finally, the networks (a) pretraining and (b) pretraining and blending using both hate comment and reply yield best results (F1: 0.52).

	
	\subsection{Differentiating between the Top-$k$\% and Bottom-$k$\% replies}
	\label{ss:topbottom}
	Although determining the incivility level of any replies to hateful comment is a worthwhile goal (Section \ref{ss:levels}),
	differentiating between the top-$k$\% and bottom-$k$\% replies according to their incivility scores may lead to better actionable knowledge.
	Replies with the highest and lowest incivility scores are more informative than the large amount of replies with high and low conversation incivility.
	Indeed, the latter have a large range of incivility scores.
	
	Table \ref{t:model-optimalk} presents the results with several $k$ values.
	We implement with one of the best performing systems from Table \ref{t:model-results} (hate comment + reply + pretraining).
	We include in the top-$k$\% and bottom-$k$\% of all replies with the threshold incivility scores.
	Results show that the smaller the $k$, the easier it is to differentiate between the two kinds of replies.
	In other words, replies with the most and least conversation incivility differ in language usage and the classifier is able to distinguish them.
	This is especially true when $k=5$.

	
	\section{Qualitative analysis}
	\label{s:erroranalysis}
	
	When determining the incivility level of a reply,
	when does our best model (Table \ref{t:model-results}) make mistakes? 
	To investigate this question,
	we manually analyze 200 random samples in which the output of the network differs from the ground truth. 
	Table \ref{t:error} exemplifies the most common error types.
	
	The most frequent error type (23\%) is \emph{Rhetorical questions},
	a finding consistent with previous work~\cite{schmidt-wiegand-2017-survey}. 
	In the example,
	the model fails to realize that the question in the hate comment is used to point out inappropriate content rather than expecting an answer.
	
	The second and third most common error types (18\% and 16\%)
	are when a reply is
	(a)~uncivil but has low incivility
	or
	(b)~civil but has high incivility.
	Using uncivil language is a counter hate strategy~\cite{DBLP:conf/icwsm/MathewSTRSMG019},
	and the model fails to recognize when doing so leads to low conversation incivility .
	Similarly, the model struggles when countering hate politely elicits additional uncivil behaviors.
	When correcting misstatements, language toxicity may increase~\cite{10.1145/3411764.3445642}.
	
	
	Sarcasm and irony are also common error types (15\%) in our task,
	consistent with the task of detecting hate~\cite{nobata2016abusive,qian-etal-2019-benchmark}.
	In the example, using sarcasm to point out a bad argument elicits further hate and the model errs.
	
	Errors may also occur (10\%) when general knowledge is required to identify hate content that does not use offensive language (e.g., calling people \emph{incels}).
	Finally, we observe that \emph{negation} appears in 8\% errors.
	In the example, negations are used to point out the flaws of generalizing.
	We hypothesize that the model fails to identify that the reply has medium conversation incivility: negation does indicate high incivility in general (Table~\ref{t:linguistic}).
	
	
	\section{Conclusions}
	Not all replies to hateful content have equal conversation incivility.
	In this paper, we work with a large dataset from Reddit and present a metric to measure conversation incivility of replies to hateful content.
	Our metric takes account into the number of both civil and uncivil comments from each user.
	Regardless of whether replies counter hateful content convincingly,
	we believe it is worthwhile to identify what kind of user-generated content attracts great attention and at the same time shapes civil discussions.
	While we make no causal claims which linguistic features could affect conversation incivility, 
	we show that the language of user-generated replies differs depending on their conversation incivility levels.
	Indeed, our analyses can recover some of the intuition: replies that use more negative and hateful words are tied with a higher incivility degree in their follow-up conversations.
	Experimental results show that pretraining and blending existing corpora yield improvements, while the task of forecasting conversation incivility is still challenging to automate.
	
	We rely on automated methods to formulate a setting that allows us to differentiate replies with high conversation incivility from low conversation incivility. 
	Our manual validation suggests that it is feasible at the level of human intuition where follow-up conversations are accessible (see Section~\ref{ss:validation}).
	The experiments that differentiate the top-$k$\% from bottom-$k$\% achieve even more encouraging results when $k$ is smaller, where follow-up conversations are not accessible (see Section~\ref{ss:topbottom}).
	Both bring up the potential of automated methods to measure and forecast conversation incivility of replies to hateful content, when they usually have at least one comment following.
	
	
	
	\section*{Limitations}
	Our work has several limitations. 
	First, our correlational analyses do not provide any insights into casual mechanism of conversation incivility, which small-scale user studies could address (see Section~\ref{s:related_work}).
	Second, we identify uncivil comments automatically with classifiers.
	These classifiers obtain good results but are not perfect.
	As a result,
	some of the original uncivil comments we work with is actually not uncivil.
	Third, we focus on forecasting conversation incivility from linguistic aspects in this work. 
	A promising line of future work could consider to combine structural and linguistic features and incorporate other factors such as positions and user identities in modeling.
	Finally, we only consider the hateful comment and the reply in our experiments.
	More complex modeling that takes into account additional context (e.g., the full thread) may be beneficial.
	We leave it in our future work.
	
	% Entries for the entire Anthology, followed by custom entries
	\bibliography{anthology,custom}
	\bibliographystyle{acl_natbib}
	
	\appendix
	
	% Start: Table 
	\begin{table}[ht!]
		\centering
		\small
		\begin{tabular}{lrrr}
			\toprule
			Subreddit & Hate  & Reply & Future\\
			\midrule
			antiwork& 	523	& 1272	& 3754\\
			politics& 	751	& 1539	& 4195\\
			conspiracy& 	996	& 1479& 	4312\\
			BlackPeopleTwitter& 	39	& 93	& 344\\
			SubredditDrama& 	1347& 	2217& 	5087\\
			DankMemes& 	1208& 	1878	& 5369\\
			modernwarfare	& 189	& 289& 	872\\
			atheism	& 731	& 1331	& 3276\\
			justneckbeardthings	& 736& 	1227& 	2739\\
			4Chan& 	894	& 1467	& 3437\\
			oblivion& 	1885& 	2782& 	4462\\
			TwoXChromosomes	& 106& 	185& 	664\\
			mensrights& 	1332& 	1897& 	5419\\
			dota2	& 215& 	379	& 925\\
			HermanCainAward& 	1126& 	1728	& 3110\\
			PurplePillDebate	& 1075& 	1517& 	4900\\
			PussyPassDenied& 	2947& 	4494& 	10168\\
			worldnews& 	786& 	1223	& 2903\\
			changemyview	& 768& 	1080& 	1937\\
			playrust	& 99& 	134& 	366\\
			MetaCanada	& 575& 	753& 	1334\\
			bindingofisaac	& 52& 	79& 	134\\
			TumblrInAction	& 125& 	185& 	512\\
			ShitPoliticsSays	& 567& 	862& 	2142\\
			KotakuInAction	& 326& 	533& 	989\\
			TrueReddit	& 288& 	425	& 1198\\
			Conservative	& 330& 	487	& 1256\\
			NoFap	& 58	& 74& 	164\\
			GenZedong	& 118& 	171	& 269\\
			PussyPass	& 599	& 790& 	1416\\
			Drama& 	228	& 382& 	741\\
			bakchodi& 	79& 	107& 	215\\
			ShitRedditSays	& 212	& 291& 	482\\
			Seduction	& 162	& 223& 	438\\
			FemaleDatingStrategy& 	117	& 204& 	375\\
			Feminism	& 48& 	67	& 123\\
			india	& 68& 	98	& 161\\
			lmGoingToHellForThis& 	113	& 136& 	224\\
			Sino	& 27& 	37& 	43 \\
			\midrule
			Total & 21845	& 34115& 	80455 \\
			\bottomrule
		\end{tabular}
		\caption{Number of hate comments, replies, and comments after the replies per subreddit.}
		\label{t:subreddits}
	\end{table}
	% End: Table 
	
	
	
	% Start: Table 
	\begin{table*}
		\small
		\centering
		\setlength{\tabcolsep}{.07in}
		\begin{tabular}{l ccc ccc ccc ccc}
			\toprule
			\multicolumn{1}{c}{} & \multicolumn{3}{c}{High} & \multicolumn{3}{c}{Medium} & \multicolumn{3}{c}{Low} & \multicolumn{3}{c}{Weighted Average} \\
			\cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} \cmidrule(lr){11-13} 
			& P & R & F1 & P & R & F1 & P & R & F1 & P & R & F1 \\
			\hline \addlinespace 
			Majority Baseline &0.00 & 	0.00&	0.00&	0.49	&1.00	&0.66	&0.00	&0.00&	0.00&	0.24	&0.49&	0.32 \\		
			reply\\
			~~~~+ pretraining with \\
			~~~~~~~~~~~Hate & 0.46&	0.33&	0.39&	0.54&	0.83	&0.65&	0.35&	0.09&	0.14	&0.47	&0.51&	0.45\\
			~~~~~~~~~~~Sentiment & 0.43&	 0.30&	0.36&	0.55&	0.67&	0.61&	0.30&	0.27&	0.29&	0.46&	0.47&	0.46\\
			~~~~~~~~~~~Sarcasm & 0.45&	0.32&	0.37&	0.54&	0.80&	0.64&	0.33&	0.11	&0.16&	0.46&	0.50&	0.45\\
			~~~~~~~~~~~Counter & 0.44&	0.37&	0.40	&0.56&	0.72	&0.63&	0.32&	0.20&	0.25&	0.47&	0.50&	0.47\\
			~~~~~~~~~~~Stance & 0.42	&0.40&	0.41	&0.54&	0.73&	0.62&	0.30&	0.11&	0.16&	0.45&	0.49&	0.45\\
			~~~~+ blending with\\
		    ~~~~~~~~~~~sentiment& 	0.40& 	0.39& 	0.40	& 0.54& 	0.73	& 0.62& 	0.34& 	0.13& 	0.19& 	0.46& 	0.49& 	0.45 \\
			~~~~~~~~~~~counter& 	0.40& 	0.38& 	0.39& 	0.55& 	0.70& 	0.61& 	0.29& 	0.16& 	0.20& 	0.45& 	0.48& 	0.45 \\
			~~~~~~~~~~~stance	& 0.42	& 0.40& 	0.41& 	0.54& 	0.73	& 0.62& 	0.30& 	0.11& 	0.16	& 0.45& 	0.49& 	0.45 \\
			~~~~+ pretraining + blending \\
			~~~~~~~~~~~sentiment& 	0.40	& 0.39	& 0.40& 	0.54& 	0.74	& 0.62	& 0.29& 	0.09& 	0.14	& 0.44& 	0.48& 	0.44 \\
			~~~~~~~~~~~counter&  	0.45& 	0.35& 	0.40& 	0.54& 	0.84& 	0.66& 	0.39& 	0.07& 	0.11& 	0.48& 	0.52	& 0.45 \\
			~~~~~~~~~~~stance& 	0.47& 	0.39	& 0.43& 	0.57& 	0.70	& 0.63	& 0.34& 	0.25& 	0.29& 	0.49& 	0.51& 	0.49 \\
			hate comment + reply\\
			~~~~+ pretraining with \\
			~~~~~~~~~~~hate	& 	0.45&	0.28&	0.35	&0.53	&0.83&	0.65&	0.34&	0.10	&0.15&	0.46&	0.50	&0.44 \\
			~~~~~~~~~~~sentiment	&0.45&	0.38&	0.41&	0.54&	0.79&	0.65&	0.36&	0.09&	0.15&	0.47&	0.51	&0.46 \\
			~~~~~~~~~~~sarcasm	&0.40&	0.39&	0.40&	0.54&	0.74	&0.62&	0.29&	0.09	&0.14&	0.44&	0.48&	0.44 \\
			~~~~~~~~~~~counter &	0.45&	0.26&	0.33&	0.54&	0.77&	0.64&	0.34&	0.22&	0.26&	0.47&	0.50&	0.46 \\
			~~~~~~~~~~~stance&	\textbf{0.52}&	\textbf{0.43}	&\textbf{0.47}&	\textbf{0.59}	&\textbf{0.77}	&\textbf{0.67}&	0.42&	0.23&	0.30	&\textbf{0.53}&	\textbf{0.55}&	\textbf{0.52} \\
			~~~~+ blending with \\
			~~~~~~~~~~~sentiment	&0.45&	0.34&	0.39&	0.54&	0.81&	0.65	&0.32	&0.09&	0.14&	0.46	&0.51&	0.45 \\
			~~~~~~~~~~~counter 	&0.43	&0.37	&0.40&	0.54&	0.79&	0.64&	0.35&	0.10 &	0.16&	0.47&	0.51&	0.46 \\
			~~~~~~~~~~~stance	&0.45&	0.35&	0.40&	0.54&	0.76&	0.66&	0.39	&0.04&	0.08&	0.48&	0.52	&0.45 \\
			~~~~+ pretraining + blending\\
			~~~~~~~~~~~sentiment&	0.42&	0.35&	0.38&	0.55&	0.74&	0.63&	0.31&	0.14&	0.19&	0.45&	0.49&	0.46 \\
			~~~~~~~~~~~counter 	&0.48&	0.43&	0.45&	0.58&	0.71	&0.64&	0.36&	0.24&	0.29&	0.50	&0.52	&0.50 \\
			~~~~~~~~~~~stance	&0.48&	0.42&	0.45&	0.59&	0.72	&0.65&	\textbf{0.38}&	\textbf{0.27}&	\textbf{0.32}&	\textbf{0.51}&	\textbf{0.53}&	\textbf{0.52} \\
			\bottomrule
			
		\end{tabular}
		\caption{Detailed results (P, R, and F) predicting whether the reply has High, Medium, and Low conversation incivility when the input is only the reply or the hate comment + reply. These results are using RoBERTa and pretrained with or blending each each related task. This table complements Table \ref{t:model-results} in the paper.}
		\label{t:detailed-results}
	\end{table*}
	% End: Table 
	
	% Start: Table 
	\begin{table*}
		\centering
		\small
		\begin{tabular}{lccccc}
			\toprule
			& Epochs & Batch size & Learning rate & Dropout  \\
			\midrule
			reply &  5 & 8 & 1e-5 & 0.5  \\
			~~~~+ blending & 5 & 8 &  1e-5 & 0.5  \\
			~~~~+ pretraining & 3 & 8 &  1e-5 & 0.5 \\
			~~~~~~~~+ blending & 5 & 8 &  1e-5 & 0.5  \\
			\bottomrule
			
		\end{tabular}
		\caption{Hyperparameters used to fine-tune RoBERTa individually for each training setting. We accept default settings for the other hyperparameters as defined in the implementation by \citet{phang2020jiant}. }
		\label{t:hyperparameters}
	\end{table*}
	% End: Table 
	
	
	\section{Ethical Considerations}
	\label{sec:appendix}
	We use the PushShift API to collect data from Reddit.\footnote{\url{https://pushshift.io/api-parameters/}} 
	The collection process is consistent with Reddit's Terms of Service. 
	We access our data through the data dumps on Google's BigQuery using Python.\footnote{\url{https://pushshift.io/
			using-bigquery-with-reddit-data/}} 
	
	Reddit can be considered a public space for discussion which differs from a private messaging service \cite{vidgen-etal-2021-introducing}. 
	Users consent to have their data made available to third parties including academics when they sign up to Reddit. 
	Existing ethical guidelines state that in this situation explicit consent is not required from each user \cite{DBLP:conf/tto/ProcterWBHEWJ19}. 
	We obfuscate user names to reduce the possibility of identifying users. 
	In compliance with Reddit's policy, we would like to make sure that our dataset will be reused for non-commercial research only.\footnote{\url{https://www.reddit.com/wiki/api-terms/}}
	
	The annotators were warned of the potential hateful content before working on our task. 
	We provide annotators with access to supporting services throughout the task.
	Annotators were compensated with 8 US\$ per hour.
	
	
	\section{Data}
	\label{sec:appendix_data}
	The number of hate comments (Hate), replies (Reply) and comments after the replies (Future) in each subreddits are detailed in Table \ref{t:subreddits}. 
	In total, there are 21,845 hate comments, 34,115 replies, and 80,455 comments after the replies from the 39 subreddits.
	
	We utilize the taxonomy by~\citet{Weld_Zhang_Althoff_2022} to investigate the language in higher-level notions of community topic and focus. 
	We hand-label communities in the following categories iteratively until reaching agreement among the author team. 
	We include News in Media-sharing as both communities are for sharing information.
	The five categories are: Discussion communities \emph{e.g., /r/changemyview, /r/antiwork}, Hobby communities \emph{e.g., /r/dota2, /r/otakuInAction}, Identity communities \emph{e.g., /r/Feminism, /r/india}, Meme communties \emph{e.g., /r/DankMemes, /r/HermanCainAward}, and Media-sharing communities \emph{e.g., /r/worldnews, /r/conspiracy}.
	
	
	
	
	\section{Detailed Results}
	\label{sec:appendix_results}
	Table \ref{t:detailed-results} presents detailed results complementing Table \ref{t:model-results} in the paper. 
	We provide Precision, Recall and weighted F1-score using each related task for pretraining and blending.
	
	
	\section{Hyperparamters and Finetuning Process}
	\label{sec:appendix_hyper}
	
	Our dataset was pre-processed by removing URLs, removing symbols, removing any additional spaces, and at the end, converting all words to lower-case. 
	The neural model takes about an hour on average to train on a single NVIDIA TITAN Xp. 
	We use the implementation by \citet{phang2020jiant} and fine-tune the RoBERTa (base architecture; 12 layers) \cite{DBLP:journals/corr/abs-1907-11692} model for each of the four training settings. 
	For each setting, we set the hyperparameters to be the same when the input is the hateful comment, the reply, or both (Table \ref{t:hyperparameters}).
	
\end{document}
